{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise au point de processus de RAG\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-openai langchain_community pymupdf yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "#from langchain.chains import RetrievalQA\n",
    "#from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "            return config\n",
    "        except yaml.YAMLError as e:\n",
    "            print(f\"Error reading YAML file: {e}\")\n",
    "            return None\n",
    "\n",
    "config = read_config(\"../secrets/config.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=config[\"embedding\"][\"azure_endpoint\"],\n",
    "    azure_deployment=config[\"embedding\"][\"azure_deployment\"],\n",
    "    openai_api_version=config[\"embedding\"][\"azure_api_version\"],\n",
    "    api_key=config[\"embedding\"][\"azure_api_key\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=config[\"chat\"][\"azure_endpoint\"],\n",
    "    azure_deployment=config[\"chat\"][\"azure_deployment\"],\n",
    "    openai_api_version=config[\"chat\"][\"azure_api_version\"],\n",
    "    api_key=config[\"chat\"][\"azure_api_key\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../samples/B4LFlaparureguydemaupassant.pdf\"\n",
    "loader = PyMuPDFLoader(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'PDFlib 5.0.3 (C++/Win32)', 'creator': 'Macromedia FlashPaper 2.01.2283.0', 'creationdate': '2006-01-05T17:19:06+01:00', 'source': '../samples/B4LFlaparureguydemaupassant.pdf', 'file_path': '../samples/B4LFlaparureguydemaupassant.pdf', 'total_pages': 4, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2009-11-21T11:58:45+08:00', 'trapped': '', 'modDate': \"D:20091121115845+08'00'\", 'creationDate': \"D:20060105171906+01'00'\", 'page': 0}, page_content='LA\\xa0PARURE\\xa0de\\xa0Guy\\xa0de\\xa0Maupassant\\xa0(nouvelle\\xa0parue\\xa0dans\\xa0le\\xa0Gaulois\\xa0le\\xa017\\xa0février\\xa01884)\\xa0\\nC\\'était\\xa0une\\xa0de\\xa0ces\\xa0jolies\\xa0et\\xa0charmantes\\xa0filles,\\xa0nées,\\xa0comme\\xa0par\\xa0une\\xa0erreur\\xa0du\\xa0destin,\\xa0dans\\xa0une\\xa0famille\\xa0\\nd\\'employés.\\xa0Elle\\xa0n\\'avait\\xa0pas\\xa0de\\xa0dot,\\xa0pas\\xa0d\\'espérance,\\xa0aucun\\xa0moyen\\xa0d\\'être\\xa0connue,\\xa0comprise,\\xa0aimée,\\xa0\\népousée\\xa0par\\xa0un\\xa0homme\\xa0riche\\xa0et\\xa0distingué\\xa0;\\xa0et\\xa0elle\\xa0se\\xa0laissa\\xa0marier\\xa0avec\\xa0un\\xa0petit\\xa0commis\\xa0du\\xa0ministère\\xa0\\nde\\xa0l\\'Instruction\\xa0publique.\\xa0\\nElle\\xa0fut\\xa0simple\\xa0ne\\xa0pouvant\\xa0être\\xa0parée,\\xa0mais\\xa0malheureusement\\xa0comme\\xa0une\\xa0déclassée,\\xa0car\\xa0les\\xa0femmes\\xa0\\nn\\'ont\\xa0point\\xa0de\\xa0caste\\xa0ni\\xa0de\\xa0race,\\xa0leur\\xa0beauté,\\xa0leur\\xa0grâce\\xa0et\\xa0leur\\xa0charme\\xa0leur\\xa0servant\\xa0de\\xa0naissance\\xa0et\\xa0de\\xa0\\nfamille.\\xa0Leur\\xa0finesse\\xa0native,\\xa0leur\\xa0instinct\\xa0d\\'élégance,\\xa0leur\\xa0souplesse\\xa0d\\'esprit,\\xa0sont\\xa0leur\\xa0seule\\xa0hiérarchie,\\xa0\\net\\xa0font\\xa0des\\xa0filles\\xa0du\\xa0peuple\\xa0les\\xa0égales\\xa0des\\xa0plus\\xa0grandes\\xa0dames.\\xa0\\nElle\\xa0souffrait\\xa0sans\\xa0cesse,\\xa0se\\xa0sentant\\xa0née\\xa0pour\\xa0toutes\\xa0les\\xa0délicatesses\\xa0et\\xa0tous\\xa0les\\xa0luxes.\\xa0Elle\\xa0souffrait\\xa0de\\xa0\\nla\\xa0pauvreté\\xa0de\\xa0son\\xa0logement,\\xa0de\\xa0la\\xa0misère\\xa0des\\xa0murs,\\xa0de\\xa0l\\'usure\\xa0des\\xa0sièges,\\xa0de\\xa0la\\xa0laideur\\xa0des\\xa0étoffes.\\xa0\\nToutes\\xa0ces\\xa0choses,dont\\xa0une\\xa0autre\\xa0femme\\xa0de\\xa0sa\\xa0caste\\xa0ne\\xa0se\\xa0serait\\xa0même\\xa0pas\\xa0aperçue,\\xa0la\\xa0torturaient\\xa0et\\xa0\\nl\\'indignaient.\\xa0La\\xa0vue\\xa0de\\xa0la\\xa0petite\\xa0Bretonne\\xa0qui\\xa0faisait\\xa0son\\xa0humble\\xa0ménage\\xa0éveillait\\xa0en\\xa0elle\\xa0des\\xa0regrets\\xa0\\ndésolés\\xa0et\\xa0des\\xa0rêves\\xa0éperdus.\\xa0Elle\\xa0songeait\\xa0aux\\xa0antichambres\\xa0muettes,\\xa0capitonnées\\xa0avec\\xa0des\\xa0tentures\\xa0\\norientales,\\xa0éclairées\\xa0par\\xa0de\\xa0hautes\\xa0torchères\\xa0de\\xa0bronze,\\xa0et\\xa0aux\\xa0deux\\xa0grands\\xa0valets\\xa0en\\xa0culotte\\xa0courte\\xa0qui\\xa0\\ndorment\\xa0dans\\xa0les\\xa0larges\\xa0fauteuils,\\xa0assoupis\\xa0par\\xa0la\\xa0chaleur\\xa0lourde\\xa0du\\xa0calorifère.\\xa0Elle\\xa0songeait\\xa0aux\\xa0grands\\xa0\\nsalons\\xa0vêtus\\xa0de\\xa0soie\\xa0ancienne,\\xa0aux\\xa0meubles\\xa0fins\\xa0portant\\xa0des\\xa0bibelots\\xa0inestimables,\\xa0et\\xa0aux\\xa0petits\\xa0salons\\xa0\\ncoquets,\\xa0parfumés,\\xa0faits\\xa0pour\\xa0la\\xa0causerie\\xa0de\\xa0cinq\\xa0heures\\xa0avec\\xa0les\\xa0amis\\xa0les\\xa0plus\\xa0intimes,\\xa0les\\xa0hommes\\xa0\\nconnus\\xa0et\\xa0recherchés\\xa0dont\\xa0toutes\\xa0les\\xa0femmes\\xa0envient\\xa0et\\xa0désirent\\xa0l\\'attention.\\xa0\\nQuand\\xa0elle\\xa0s\\'asseyait,\\xa0pour\\xa0dîner,\\xa0devant\\xa0la\\xa0table\\xa0ronde\\xa0couverte\\xa0d\\'une\\xa0nappe\\xa0de\\xa0trois\\xa0jours,\\xa0en\\xa0face\\xa0de\\xa0\\nson\\xa0mari\\xa0qui\\xa0découvrait\\xa0la\\xa0soupière\\xa0en\\xa0déclarant\\xa0d\\'un\\xa0air\\xa0enchanté\\xa0:\\xa0\"\\xa0Ah\\xa0!\\xa0le\\xa0bon\\xa0pot\\xadau\\xadfeu\\xa0!\\xa0je\\xa0ne\\xa0sais\\xa0\\nrien\\xa0de\\xa0meilleur\\xa0que\\xa0cela...\"\\xa0elle\\xa0songeait\\xa0aux\\xa0dîners\\xa0fins,\\xa0aux\\xa0argenteries\\xa0reluisantes,\\xa0aux\\xa0tapisseries\\xa0\\npeuplant\\xa0les\\xa0murailles\\xa0de\\xa0personnages\\xa0anciens\\xa0et\\xa0d\\'oiseaux\\xa0étranges\\xa0au\\xa0milieu\\xa0d\\'une\\xa0forêt\\xa0de\\xa0féerie\\xa0;\\xa0\\nelle\\xa0songeait\\xa0aux\\xa0plats\\xa0exquis\\xa0servis\\xa0en\\xa0des\\xa0vaisselles\\xa0merveilleuses,\\xa0aux\\xa0galanteries\\xa0chuchotées\\xa0et\\xa0\\nécoutées\\xa0 avec\\xa0 un\\xa0 sourire\\xa0 de\\xa0 sphinx,\\xa0 tout\\xa0 en\\xa0 mangeant\\xa0 la\\xa0 chair\\xa0 rose\\xa0 d\\'une\\xa0 truite\\xa0 ou\\xa0 des\\xa0 ailes\\xa0 de\\xa0\\ngélinotte.\\xa0\\nElle\\xa0n\\'avait\\xa0pas\\xa0de\\xa0toilettes,\\xa0pas\\xa0de\\xa0bijoux,rien.\\xa0Et\\xa0elle\\xa0n\\'aimait\\xa0que\\xa0cela\\xa0;\\xa0elle\\xa0se\\xa0sentait\\xa0faite\\xa0pour\\xa0cela.\\xa0\\nElle\\xa0eût\\xa0tant\\xa0désiré\\xa0plaire,\\xa0être\\xa0enviée,\\xa0être\\xa0séduisante\\xa0et\\xa0recherchée.\\xa0\\nElle\\xa0avait\\xa0une\\xa0amie\\xa0riche,\\xa0une\\xa0camarade\\xa0de\\xa0couvent\\xa0qu\\'elle\\xa0ne\\xa0voulait\\xa0plus\\xa0aller\\xa0voir,\\xa0tant\\xa0elle\\xa0souffrait\\xa0\\nen\\xa0 revenant.\\xa0 Et\\xa0 elle\\xa0 pleurait\\xa0 pendant\\xa0 des\\xa0 jours\\xa0 entiers,\\xa0 de\\xa0 chagrin,\\xa0 de\\xa0 regret,\\xa0 de\\xa0 désespoir\\xa0 et\\xa0 de\\xa0\\ndétresse.\\xa0\\nOr,\\xa0un\\xa0soir,\\xa0son\\xa0mari\\xa0rentra,\\xa0l\\'air\\xa0glorieux,\\xa0et\\xa0tenant\\xa0à\\xa0la\\xa0main\\xa0une\\xa0large\\xa0enveloppe.\\xa0\\n\"Tiens,\\xa0dit\\xadil,\\xa0voici\\xa0quelque\\xa0chose\\xa0pour\\xa0toi.\"\\xa0\\nElle\\xa0déchira\\xa0vivement\\xa0le\\xa0papier\\xa0et\\xa0en\\xa0tira\\xa0une\\xa0carte\\xa0imprimée\\xa0qui\\xa0portait\\xa0ces\\xa0mots\\xa0:\\xa0\\n\"\\xa0Le\\xa0ministre\\xa0de\\xa0l\\'Instruction\\xa0publique\\xa0et\\xa0Mme\\xa0Georges\\xa0Ramponneau\\xa0prient\\xa0M.\\xa0et\\xa0Mme\\xa0Loisel\\xa0de\\xa0leur\\xa0\\nfaire\\xa0honneur\\xa0de\\xa0venir\\xa0passer\\xa0la\\xa0soirée\\xa0à\\xa0l\\'hôtel\\xa0du\\xa0ministère,\\xa0le\\xa0lundi\\xa018\\xa0janvier.\"\\xa0\\nAu\\xa0lieu\\xa0d\\'être\\xa0ravie,\\xa0comme\\xa0l\\'espérait\\xa0son\\xa0mari,\\xa0elle\\xa0jeta\\xa0avec\\xa0dépit\\xa0l\\'invitation\\xa0sur\\xa0la\\xa0table,\\xa0murmurant\\xa0:\\xa0\\n«\\xa0 Que\\xa0veux\\xadtu\\xa0que\\xa0je\\xa0fasse\\xa0de\\xa0cela\\xa0?\\xa0»\\xa0\\n\\xad\\xa0Mais,\\xa0ma\\xa0chérie,\\xa0je\\xa0pensais\\xa0que\\xa0tu\\xa0serais\\xa0contente.\\xa0Tu\\xa0ne\\xa0sors\\xa0jamais,\\xa0et\\xa0c\\'est\\xa0une\\xa0occasion,\\xa0cela,\\xa0une\\xa0\\nbelle\\xa0!\\xa0J\\'ai\\xa0eu\\xa0une\\xa0peine\\xa0infinie\\xa0à\\xa0l\\'obtenir.\\xa0Tout\\xa0le\\xa0monde\\xa0en\\xa0veut\\xa0;\\xa0c\\'est\\xa0très\\xa0recherché\\xa0et\\xa0on\\xa0n\\'en\\xa0donne\\xa0\\npas\\xa0beaucoup\\xa0aux\\xa0employés.\\xa0Tu\\xa0verras\\xa0là\\xa0tout\\xa0le\\xa0monde\\xa0officiel.\\xa0\"\\xa0\\nElle\\xa0le\\xa0regardait\\xa0d\\'un\\xa0oeil\\xa0irrité,\\xa0et\\xa0elle\\xa0déclara\\xa0avec\\xa0impatience\\xa0:\\xa0\"Que\\xa0veux\\xadtu\\xa0que\\xa0je\\xa0me\\xa0mette\\xa0sur\\xa0le\\xa0\\ndos\\xa0pour\\xa0aller\\xa0là\\xa0?\"\\xa0\\nIl\\xa0n\\'y\\xa0avait\\xa0pas\\xa0songé\\xa0;\\xa0il\\xa0balbutia\\xa0:\\xa0\"\\xa0Mais\\xa0la\\xa0robe\\xa0avec\\xa0laquelle\\xa0tu\\xa0vas\\xa0au\\xa0théâtre.\\xa0Elle\\xa0me\\xa0semble\\xa0très\\xa0\\nbien,\\xa0à\\xa0moi...\"\\xa0\\nIl\\xa0 se\\xa0 tut,\\xa0 stupéfait,\\xa0 éperdu,\\xa0 en\\xa0 voyant\\xa0 que\\xa0 sa\\xa0 femme\\xa0 pleurait.\\xa0 Deux\\xa0 grosses\\xa0 larmes\\xa0 descendaient\\xa0\\nlentement\\xa0des\\xa0coins\\xa0des\\xa0yeux\\xa0vers\\xa0les\\xa0coins\\xa0de\\xa0la\\xa0bouche\\xa0;\\xa0il\\xa0bégaya\\xa0:\\xa0\"Qu\\'as\\xadtu\\xa0?\\xa0Qu\\'as\\xadtu\\xa0?\"\\xa0\\nMais,\\xa0par\\xa0un\\xa0effort\\xa0violent,elle\\xa0avait\\xa0dompté\\xa0sa\\xa0peine\\xa0et\\xa0elle\\xa0répondit\\xa0d\\'une\\xa0voix\\xa0calme\\xa0en\\xa0essuyant\\xa0ses\\xa0\\njoues\\xa0humides\\xa0:\\xa0\"\\xa0Rien.\\xa0Seulement\\xa0je\\xa0n\\'ai\\xa0pas\\xa0de\\xa0toilette\\xa0et\\xa0par\\xa0conséquent\\xa0je\\xa0ne\\xa0peux\\xa0aller\\xa0à\\xa0cette\\xa0fête.\\xa0\\nDonne\\xa0ta\\xa0carte\\xa0à\\xa0quelque\\xa0collègue\\xa0dont\\xa0la\\xa0femme\\xa0sera\\xa0mieux\\xa0nippée\\xa0que\\xa0moi.\"\\xa0\\nIl\\xa0 était\\xa0 désolé.\\xa0 Il\\xa0 reprit\\xa0 :\\xa0 \"\\xa0 Voyons,\\xa0 Mathilde.\\xa0 Combien\\xa0 cela\\xa0 coûterait\\xadil,\\xa0 une\\xa0 toilette\\xa0 convenable,\\xa0 qui\\xa0\\npourrait\\xa0te\\xa0servir\\xa0encore\\xa0en\\xa0d\\'autres\\xa0occasions,\\xa0quelque\\xa0chose\\xa0de\\xa0très\\xa0simple\\xa0?\"\\xa0\\nElle\\xa0réfléchit\\xa0quelques\\xa0secondes,\\xa0établissant\\xa0ses\\xa0comptes\\xa0et\\xa0songeant\\xa0aussi\\xa0à\\xa0la\\xa0somme\\xa0qu\\'elle\\xa0pouvait\\xa0\\ndemander\\xa0sans\\xa0s\\'attirer\\xa0un\\xa0refus\\xa0immédiat\\xa0et\\xa0une\\xa0exclamation\\xa0effarée\\xa0du\\xa0commis\\xa0économe.\\xa0\\nEnfin\\xa0elle\\xa0répondit\\xa0en\\xa0hésitant\\xa0:\\xa0\"\\xa0Je\\xa0ne\\xa0sais\\xa0pas\\xa0au\\xa0juste,\\xa0mais\\xa0il\\xa0me\\xa0semble\\xa0qu\\'avec\\xa0quatre\\xa0cents\\xa0francs\\xa0\\nje\\xa0pourrais\\xa0arriver.\"\\xa0\\nIl\\xa0 avait\\xa0 un\\xa0 peu\\xa0 pâli,car\\xa0 il\\xa0 réservait\\xa0 juste\\xa0 cette\\xa0 somme\\xa0pour\\xa0 acheter\\xa0 un\\xa0 fusil\\xa0 et\\xa0s\\'offrir\\xa0 des\\xa0 parties\\xa0 de\\xa0\\nchasse,\\xa0l\\'été\\xa0suivant,\\xa0dans\\xa0la\\xa0plaine\\xa0de\\xa0Nanterre,\\xa0avec\\xa0quelques\\xa0amis\\xa0qui\\xa0allaient\\xa0tirer\\xa0des\\xa0alouettes,\\xa0\\npar\\xa0là,\\xa0le\\xa0dimanche.\\xa0\\nIl\\xa0dit\\xa0cependant\\xa0:\\xa0\"Soit.\\xa0Je\\xa0te\\xa0donne\\xa0quatre\\xa0cents\\xa0francs.\\xa0Mais\\xa0tâche\\xa0d\\'avoir\\xa0une\\xa0belle\\xa0robe.\"')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interrogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(store, question: str):\n",
    "    retrieved_docs = store.similarity_search(question)\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain import hub\n",
    "\n",
    "# print(hub.pull(\"rlm/rag-prompt\").messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qa_messages(question: str, context: str) -> list[str]:\n",
    "    messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an assistant for question-answering tasks.\",\n",
    "    ),\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"Use the following pieces of retrieved context to answer the question.\n",
    "        If you don't know the answer, just say that you don't know.\n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        {}\"\"\".format(context),\n",
    "    ),\n",
    "    (  \n",
    "        \"user\",\n",
    "        question\n",
    "    ),]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"comment s'appelle l'amie de madame Loisel\"\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieve(vector_store, question))\n",
    "messages = build_qa_messages(question, docs_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'amie de Madame Loisel s'appelle Jeanne Forestier.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application à un autre fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la liste des ouvrages mentionnés dans le document :\n",
      "\n",
      "1. Manuel Delanda, \"Philosophy and Simulation: The Emergence of Synthetic Reason,\" London: Continuum, 2011.\n",
      "2. Matteo Pasquinelli, “Italian Operaismo and the Information Machine,” Theory, Culture & Society, first published on February 2, 2014.\n",
      "3. Hans Belting, \"Florenz und Bagdad: Eine westöstliche Geschichte des Blicks,\" Munich: Beck Verlag, 2008.\n",
      "4. Gabriel Tarde, \"The Laws of Imitation,\" New York: Holt, 1903 [first published in French in 1890].\n",
      "5. Forensic Architecture (ed.), \"Forensis: The Architecture of Public Truth,\" Berlin: Sternberg Press, 2014.\n",
      "6. Eyal and Ines Weizman, “Before and After: Documenting the Architecture of Disaster,” Moscow and London: Strelka Press, 2013.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../samples/Anomaly_Detection_The_Mathematization_of.pdf\"\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "new_vector_store = InMemoryVectorStore(embeddings)\n",
    "_ = new_vector_store.add_documents(documents=all_splits)\n",
    "question = \"liste les ouvrages mentionnés dans le document\"\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieve(new_vector_store, question))\n",
    "messages = build_qa_messages(question, docs_content)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les auteurs mentionnés dans le document sont Manuel Delanda, Matteo Pasquinelli, Hans Belting, Clemens von Wedemeyer, Gabriel Tarde, Forensic Architecture (éditeur), et Georges Canguilhem.\n"
     ]
    }
   ],
   "source": [
    "question = \"liste les auteurs mentionnés dans le document\"\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieve(new_vector_store, question))\n",
    "messages = build_qa_messages(question, docs_content)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les organisations mentionnées dans le document sont : \n",
      "\n",
      "1. Une organisation militaire (en lien avec l'utilisation du programme de détection de menaces).\n",
      "2. DARPA (en lien avec le programme ADAMS: Anomaly Detection at Multiple Scale).\n",
      "3. Forensic Architecture.\n"
     ]
    }
   ],
   "source": [
    "question = \"liste les organisations mentionnées dans le document\"\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieve(new_vector_store, question))\n",
    "messages = build_qa_messages(question, docs_content)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_doc(extract: str) -> str:\n",
    "    messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a librarian extracting metadata from documents.\",\n",
    "    ),\n",
    "    (\n",
    "        \"user\",\n",
    "        \"\"\"Extract from the content the following metadata.\n",
    "        Answer 'unknown' if you cannot find or generate the information.\n",
    "        Metadata list:\n",
    "        - title\n",
    "        - author\n",
    "        - source\n",
    "        - type of content (e.g. scientific paper, litterature, news, etc.)\n",
    "        - language\n",
    "        - themes as a list of keywords\n",
    "\n",
    "        <content>\n",
    "        {}\n",
    "        </content>\n",
    "        \"\"\".format(extract),\n",
    "    ),]\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = \"../samples/article_nature.pdf\"\n",
    "loader = PyMuPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "extract = '\\n\\n'.join([split.page_content for split in all_splits[:min(10, len(all_splits))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- title: Effective lung nodule detection using deep CNN with dual attention mechanisms\n",
      "- author: Zia Ur Rehman, Yan Qiang, Long Wang, Yiwei Shi, Qianqian Yang, Saeed Ullah Khattak, Rukhma Aftab & Juanjuan Zhao\n",
      "- source: Scientific Reports\n",
      "- type of content: scientific paper\n",
      "- language: English\n",
      "- themes: ['lung cancer', 'deep learning', 'convolutional neural networks', 'dual attention mechanisms', 'medical imaging', 'cancer detection', 'computer-aided diagnosis']\n"
     ]
    }
   ],
   "source": [
    "print(get_meta_doc(extract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
